{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from utils import networks\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from gan2shape.renderer.renderer import Renderer\n",
    "from torchvision.utils import save_image\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dAhqUJCpxUD",
    "outputId": "dfe55a7a-1c36-4ca5-f1a6-de82daba3dea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recon_im: torch.Size([2, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "pretrain_pth_path = \"pretrain.pth\"\n",
    "image_path = \"test.png\"\n",
    "image_name = image_path.split(\"/\")[-1]\n",
    "image_size = 128\n",
    "ckpt = torch.load(pretrain_pth_path, map_location=\"cpu\")\n",
    "max_depth = 1.1\n",
    "min_depth = 0.9\n",
    "gn_base = 8 if image_size >= 128 else 16\n",
    "nf = max(4096 // image_size, 16)\n",
    "flip1 = [False, True, True, True]\n",
    "flip3 = [True, True, True, True]\n",
    "mode = 'step3' \n",
    "depth_rescaler = lambda d: (1+d)/2 *max_depth + (1-d)/2 *min_depth\n",
    "add_mean_V = True\n",
    "xyz_rotation_range =60\n",
    "xy_translation_range = 0.1\n",
    "z_translation_range = 0\n",
    "add_mean_L = True\n",
    "use_mask = False\n",
    "border_depth = (0.7*max_depth + 0.3*min_depth)\n",
    "\n",
    "def init_VL_sampler():\n",
    "        from torch.distributions.multivariate_normal import MultivariateNormal as MVN\n",
    "        view_mvn_path = 'celeba_view_mvn.pth'\n",
    "        light_mvn_path = 'celeba_light_mvn.pth'\n",
    "        view_mvn = torch.load(view_mvn_path)\n",
    "        light_mvn = torch.load(light_mvn_path)\n",
    "        return view_mvn['mean'].cuda(),light_mvn['mean'].cuda(),MVN(view_mvn['mean'].cuda(), view_mvn['cov'].cuda()),MVN(light_mvn['mean'].cuda(), light_mvn['cov'].cuda())\n",
    "\n",
    "netDepth = networks.EDDeconv(cin=3, cout=1, size=image_size, nf=nf, gn_base=gn_base, zdim=256, activation=None)\n",
    "netDepth.load_state_dict(ckpt[\"netD\"])\n",
    "netDepth.cuda()\n",
    "netDepth.eval()\n",
    "netalbedo = networks.EDDeconv(cin=3, cout=3, size=image_size, nf=nf, gn_base=gn_base, zdim=256)\n",
    "netalbedo.load_state_dict(ckpt[\"netA\"])\n",
    "netalbedo.cuda()\n",
    "netalbedo.eval()\n",
    "netView = networks.Encoder(cin=3, cout=6, size=image_size, nf=nf)\n",
    "netView.load_state_dict(ckpt[\"netV\"])\n",
    "netView.cuda()\n",
    "netView.eval()\n",
    "netLight = networks.Encoder(cin=3, cout=4, size=image_size, nf=nf)\n",
    "netLight.load_state_dict(ckpt[\"netL\"])\n",
    "netLight.cuda()\n",
    "netLight.eval()\n",
    "renderer = Renderer(ckpt, image_size)\n",
    "view_mean,light_mean,view_mvn,light_mvn = init_VL_sampler()\n",
    "\n",
    "transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "image = Image.open(image_path)\n",
    "image = transform(image).unsqueeze(0).cuda()\n",
    "image = image * 2 - 1\n",
    "image.cuda()\n",
    "\n",
    "b = 1\n",
    "h, w = image_size, image_size\n",
    "## predict depth\n",
    "depth_raw = netDepth(image).squeeze(1)  # 1xHxW\n",
    "depth = depth_raw - depth_raw.view(1,-1).mean(1).view(1,1,1)\n",
    "depth = depth.tanh()\n",
    "depth = depth_rescaler(depth)\n",
    "depth_border = torch.zeros(1,h,w-4).cuda()\n",
    "depth_border = nn.functional.pad(depth_border, (2,2), mode='constant', value=1.02)\n",
    "depth = depth*(1-depth_border) + depth_border *border_depth\n",
    "if (flip3 and mode == 'step3') or flip1:\n",
    "        depth = torch.cat([depth, depth.flip(2)], 0)\n",
    "\n",
    "## predict viewpoint transformation\n",
    "view = netView(image)\n",
    "if add_mean_V:\n",
    "    view = view + view_mean.unsqueeze(0)\n",
    "\n",
    "view_trans = torch.cat([\n",
    "    view[:,:3] *math.pi/180 * xyz_rotation_range,\n",
    "    view[:,3:5] * xy_translation_range,\n",
    "    view[:,5:] *z_translation_range], 1)\n",
    "\n",
    "if flip3 and mode == 'step3':\n",
    "    view_trans = view.repeat(2,1)\n",
    "renderer.set_transform_matrices(view_trans)\n",
    "\n",
    "## predict albedo\n",
    "albedo = netalbedo(image)  # 1x3xHxW\n",
    "if (flip3 and mode == 'step3') or flip1:\n",
    "    albedo = torch.cat([albedo, albedo.flip(3)], 0)  # flip\n",
    "save_image(albedo, f'./depth/recon_albedo.png')\n",
    "\n",
    "## predict lighting\n",
    "light = netLight(image)  # Bx4\n",
    "if add_mean_L:\n",
    "   light = light + light_mean.unsqueeze(0)\n",
    "if (flip3 and mode == 'step3') or flip1:\n",
    "   light = light.repeat(2,1)  # Bx4\n",
    "light_a = light[:,:1] /2+0.5  # ambience term\n",
    "light_b = light[:,1:2] /2+0.5  # diffuse term\n",
    "light_dxy = light[:,2:]\n",
    "light_d = torch.cat([light_dxy, torch.ones(light.size(0),1).cuda()], 1)\n",
    "light_d = light_d / ((light_d**2).sum(1, keepdim=True))**0.5  # diffuse light direction\n",
    "\n",
    "## shading\n",
    "normal = renderer.get_normal_from_depth(depth)\n",
    "diffuse_shading = (normal * light_d.view(-1,1,1,3)).sum(3).clamp(min=0).unsqueeze(1)\n",
    "shading = light_a.view(-1,1,1,1) + light_b.view(-1,1,1,1)*diffuse_shading\n",
    "texture = (albedo/2+0.5) * shading *2-1\n",
    "\n",
    "\n",
    "recon_depth = renderer.warp_canon_depth(depth)\n",
    "recon_normal = renderer.get_normal_from_depth(recon_depth)\n",
    "save_image(recon_depth, f'./depth/recon_depth.png')\n",
    "\n",
    "grid_2d_from_canon = renderer.get_inv_warped_2d_grid(recon_depth)\n",
    "margin = (max_depth - min_depth) /2\n",
    "recon_im_mask = (recon_depth < max_depth+margin).float()  # invalid border pixels have been clamped at max_depth+margin\n",
    "if (flip3 and mode == 'step3') or flip1:\n",
    "        recon_im_mask = recon_im_mask[:b] * recon_im_mask[b:]\n",
    "        recon_im_mask = recon_im_mask.repeat(2,1,1)\n",
    "recon_im_mask = recon_im_mask.unsqueeze(1).detach()\n",
    "recon_im = nn.functional.grid_sample(texture, grid_2d_from_canon, mode='bilinear').clamp(min=-1, max=1)\n",
    "print(\"recon_im:\",recon_im.shape)\n",
    "save_image(recon_im[0], f'./depth/recon_img.png', nrow=1)\n",
    "save_image(recon_im[1], f'./depth/recon_img1.png', nrow=1)\n",
    "with torch.no_grad():\n",
    "    _depth, _texture, _view = depth[0,None], texture[0,None], view_trans[0,None]\n",
    "    num_p, num_y = 5, 9  # number of pitch and yaw angles to sample\n",
    "    max_y = 70\n",
    "    maxr = [20, max_y]\n",
    "    # sample viewpoints\n",
    "    im_rotate = renderer.render_view(_texture, _depth, maxr=maxr, nsample=[num_p,num_y])[0]\n",
    "    im_rotate = im_rotate/2+0.5\n",
    "    for i in range(im_rotate.size(0)):\n",
    "      save_image(im_rotate[i,None], f'./result/{image_name}_im_rotate_stage3_{i:03}.png', nrow=1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of GAN2Shape Colab Demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
